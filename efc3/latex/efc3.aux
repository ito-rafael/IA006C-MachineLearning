\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Source files}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{All code cited and all figures showed here can be found at the following GitHub repository:\\ \url  {https://github.com/ito-rafael/IA006C-MachineLearning/tree/master/efc3}\\ In this repository, one can found the following files:\\}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``MLP'' implements a multi-layer perceptron network used for binary classification. Here, we use different numbers of neurons, plot the decision regions and also calculate some metrics to evaluate the overall performance.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``SVM'' uses the same dataset and also implements a binary classifier, but it does this using a support vector machine. Here, we plot the decision regions as well, but this time we vary the value of the hyperparameter C and also the value gamma used in a RBF kernel.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part I - Error backpropagation}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Part II - Binary Classification with MLP and SVMs}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multi-layer Perceptron (MLP)}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}a) MLP network}{3}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The MLP network was implemented with the scikit-learn ``MLPClassifier'' class. This model optimizes the log-loss function. The parameters of the network were constantly modified in order to tune the classifier. The final configuration was:}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameters:}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{To determine the number of neurons, we trained the network with different number of neurons, choosing a reasonable value with a good generalization (not few neurons, avoiding an underfitting, not too many, avoiding overfitting, this will be clear in Figure \ref  {fig:mlp-decision_regions_N_neurons}). The batch size was chosen to be 200, which represents 20\% of the training data, this means that for each epoch we would have 5 updates in the algorithm. The rectifier function was chosen to be the activation function, so each unit is what is called a ReLU. The solver used is adam, with parameters values of 0.9, 0.95 and 1e-8 for beta\_1, beta\_2 and epsilon, respectively. We are also using a seed to generate pseudo-random numbers, so the results showed here can all be exactly replicated. Another important thing to note is that the max\_iter parameter is 1 because we are training the network iteration by iteration. This is done in order to plot the progression of the cost function during the training and during the validation. For this to work, we use warm\_start as True. The tolerance used is 1e-3 and the alpha (regularization coefficient) is 0.01.}{3}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training set\relax }}{4}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mlp-raw_data}{{2}{4}{Training set\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Progression of cost function in training and validation\relax }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mlp-training_validation_loss}{{3}{4}{Progression of cost function in training and validation\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{The network was trained with the parameters showed previously. Here we explored the early stopping as a cross validation method along the validation set. The training data can be seen in Figure \ref  {fig:mlp-raw_data} and the progression of the cost function for training and validation can be seen in Figure \ref  {fig:mlp-training_validation_loss}.}{4}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ROC curve for 50 neurons\relax }}{5}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mlp-roc_curve}{{4}{5}{ROC curve for 50 neurons\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{In Figure \ref  {fig:mlp-roc_curve} we can see the ROC curve for this classifier.}{5}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}b) MLP decision regions}{5}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Decision regions for MLP with 50 neurons\relax }}{5}{figure.caption.14}\protected@file@percent }
\newlabel{fig:mlp-decision_regions}{{5}{5}{Decision regions for MLP with 50 neurons\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{A function was created to plot the decision regions. This function receives as parameters the dataset and the model. The dataset is used only to define the limits of the plot and the predict method of the model is used to determine the label of each division of the grid. The Figure \ref  {fig:mlp-decision_regions} shows the decision regions for this MLP classifier.}{5}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}c) Applying MLP model to test set}{6}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MLP predicted output for test set\relax }}{6}{figure.caption.16}\protected@file@percent }
\newlabel{fig:mlp-test_set_output}{{6}{6}{MLP predicted output for test set\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{We then applied the trained network in the test set. The output generated can be seen in Figure \ref  {fig:mlp-test_set_output}. The accuracy was 0.879, indicating an error of 12.1\%.}{6}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}d) Experimenting network with different number of neurons}{6}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{fig:mlp-5_neurons}{{7a}{6}{5 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-5_neurons}{{a}{6}{5 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-25_neurons}{{7b}{6}{25 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-25_neurons}{{b}{6}{25 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-75_neurons}{{7c}{6}{75 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-75_neurons}{{c}{6}{75 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-100_neurons}{{7d}{6}{100 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-100_neurons}{{d}{6}{100 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-300_neurons}{{7e}{6}{300 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-300_neurons}{{e}{6}{300 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-1000_neurons}{{7f}{6}{1000 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-1000_neurons}{{f}{6}{1000 neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-5000_neurons}{{7g}{6}{5000 neurons\relax }{figure.caption.18}{}}
\newlabel{sub@fig:mlp-5000_neurons}{{g}{6}{5000 neurons\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Decision regions for MLP with different number of neurons\relax }}{6}{figure.caption.18}\protected@file@percent }
\newlabel{fig:mlp-decision_regions_N_neurons}{{7}{6}{Decision regions for MLP with different number of neurons\relax }{figure.caption.18}{}}
\newlabel{fig:mlp-5_roc}{{8a}{7}{5 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-5_roc}{{a}{7}{5 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-25_roc}{{8b}{7}{25 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-25_roc}{{b}{7}{25 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-75_roc}{{8c}{7}{75 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-75_roc}{{c}{7}{75 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-100_roc}{{8d}{7}{100 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-100_roc}{{d}{7}{100 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-300_roc}{{8e}{7}{300 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-300_roc}{{e}{7}{300 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-1000_roc}{{8f}{7}{1000 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-1000_roc}{{f}{7}{1000 neurons\relax }{figure.caption.19}{}}
\newlabel{fig:mlp-5000_roc}{{8g}{7}{5000 neurons\relax }{figure.caption.19}{}}
\newlabel{sub@fig:mlp-5000_roc}{{g}{7}{5000 neurons\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces ROC curves for MLP with different number of neurons\relax }}{7}{figure.caption.19}\protected@file@percent }
\newlabel{fig:mlp-ROC_N_neurons}{{8}{7}{ROC curves for MLP with different number of neurons\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{We then tried networks with different number of neurons. 5, 25, 75, 100, 300, 1000 and 5000 neurons were utilized. Figure \ref  {fig:mlp-decision_regions_N_neurons} shows the decisions regions for these classifiers and Figure \ref  {fig:mlp-ROC_N_neurons} shows the ROC curves.}{7}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As expected, for networks with few neurons (5 or 25), the model does not have flexibility for separate the classes (underfitting). The opposite is also true, the network with 5000 neurons ``learned'' too much from the dataset compromising the generalization (overfitting). The networks with 75, 100 and 300 neurons presented satisfied results.}{7}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of ROC curves\relax }}{7}{figure.caption.22}\protected@file@percent }
\newlabel{fig:mlp-roc_comparasion}{{9}{7}{Comparison of ROC curves\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{In Figure \ref  {fig:mlp-roc_comparasion} we plot all ROC curves under a single plot. This way we can compare the classifiers.}{8}{figure.caption.22}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of results between networks with different number of neurons\relax }}{8}{table.caption.24}\protected@file@percent }
\newlabel{tab:results}{{1}{8}{Comparison of results between networks with different number of neurons\relax }{table.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{We also present in Table \ref  {tab:results} other metrics used to evaluate the classifiers. Accuracy in test set, F1-score and area under the ROC curve were calculated. Since the dataset is quite balanced, we are using the simple F1-score, instead of micro and macro.}{8}{table.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Support Vector Machine (SVM)}{9}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}e) SVM decision regions and support vectors}{9}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Decision regions\relax }}{9}{figure.caption.26}\protected@file@percent }
\newlabel{fig:svm-decision_regions}{{10}{9}{Decision regions\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Support vectors\relax }}{9}{figure.caption.27}\protected@file@percent }
\newlabel{fig:svm-support_vectors}{{11}{9}{Support vectors\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {paragraph}{We now developed a SVM to do the classification. To find the hyperparameter C we used cross-validation sweeping C from 0.1 until 10 in steps of 0.1. The C for minimum error in validation set found was 8.3. The Figure \ref  {fig:svm-decision_regions} shows the decision regions for this value of C plotted with the same function used in the MLP. In Figure \ref  {fig:svm-support_vectors} we can see the support vectors identified.}{9}{figure.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}f) Applying SVM model to test set}{10}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SVM predicted output for test set\relax }}{10}{figure.caption.29}\protected@file@percent }
\newlabel{fig:svm-test_set_output}{{12}{10}{SVM predicted output for test set\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {paragraph}{We then used the model along with the test set. Figure \ref  {fig:svm-test_set_output} shows the output generated for each data. The accuracy obtained was 0.877, indicating an error of 12.3\%.}{10}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}g) Experimenting different kernel parameters and values of C}{11}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{fig:svm-decision_region_C_0d1}{{13a}{11}{Decision regions for C = 0.1\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-decision_region_C_0d1}{{a}{11}{Decision regions for C = 0.1\relax }{figure.caption.31}{}}
\newlabel{fig:svm-support_vectors_C_0d1}{{13b}{11}{Support vectors for C = 0.1\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-support_vectors_C_0d1}{{b}{11}{Support vectors for C = 0.1\relax }{figure.caption.31}{}}
\newlabel{fig:svm-decision_region_C_1}{{13c}{11}{Decision regions for C = 1.0\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-decision_region_C_1}{{c}{11}{Decision regions for C = 1.0\relax }{figure.caption.31}{}}
\newlabel{fig:svm-support_vectors_C_1}{{13d}{11}{Support vectors for C = 1.0\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-support_vectors_C_1}{{d}{11}{Support vectors for C = 1.0\relax }{figure.caption.31}{}}
\newlabel{fig:svm-decision_region_C_20}{{13e}{11}{Decision regions for C = 20\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-decision_region_C_20}{{e}{11}{Decision regions for C = 20\relax }{figure.caption.31}{}}
\newlabel{fig:svm-support_vectors_C_20}{{13f}{11}{Support vectors for C = 20\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-support_vectors_C_20}{{f}{11}{Support vectors for C = 20\relax }{figure.caption.31}{}}
\newlabel{fig:svm-decision_region_C_1000}{{13g}{11}{Decision regions for C = 1000\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-decision_region_C_1000}{{g}{11}{Decision regions for C = 1000\relax }{figure.caption.31}{}}
\newlabel{fig:svm-support_vectors_C_1000}{{13h}{11}{Support vectors for C = 1000\relax }{figure.caption.31}{}}
\newlabel{sub@fig:svm-support_vectors_C_1000}{{h}{11}{Support vectors for C = 1000\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Decision regions and respective support vectors for different values of hyperparameter C\relax }}{11}{figure.caption.31}\protected@file@percent }
\newlabel{fig:svm-decision_regions_support_vectos_C}{{13}{11}{Decision regions and respective support vectors for different values of hyperparameter C\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{We now used different values of the hyperparameter C to see what impact this would cause in the classifier. In Figure \ref  {fig:svm-decision_regions_support_vectos_C} we can see the decision regions and the respective support vectors for values of C equal to 0.1, 1, 20 and 1000.}{11}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We can see that for small values of C, we don't give flexibility to the model, suggesting an underfitting. In fact, what we are doing is giving a small weight for the points that invade the border. This produces a rough division in the dataset. We can see that the support vectors are all data near the border. The opposite, occurs for example with C = 1000. We are penalizing so much the points that invade the border, that the model will try to fit all data from the training set, leading to an overfitting. This will try to keep only the data very close to the border to be the support vectors.}{12}{section*.32}\protected@file@percent }
\newlabel{fig:svm-decision_region_gamma_0d1}{{14a}{12}{Decision regions for gamma = 0.1\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-decision_region_gamma_0d1}{{a}{12}{Decision regions for gamma = 0.1\relax }{figure.caption.34}{}}
\newlabel{fig:svm-support_vectors_gamma_0d1}{{14b}{12}{Support vectors for gamma = 0.1\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-support_vectors_gamma_0d1}{{b}{12}{Support vectors for gamma = 0.1\relax }{figure.caption.34}{}}
\newlabel{fig:svm-decision_region_gamma_1}{{14c}{12}{Decision regions for gamma = 1.0\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-decision_region_gamma_1}{{c}{12}{Decision regions for gamma = 1.0\relax }{figure.caption.34}{}}
\newlabel{fig:svm-support_vectors_gamma_1}{{14d}{12}{Support vectors for gamma = 1.0\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-support_vectors_gamma_1}{{d}{12}{Support vectors for gamma = 1.0\relax }{figure.caption.34}{}}
\newlabel{fig:svm-decision_region_gamma_10}{{14e}{12}{Decision regions for gamma = 10\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-decision_region_gamma_10}{{e}{12}{Decision regions for gamma = 10\relax }{figure.caption.34}{}}
\newlabel{fig:svm-support_vectors_gamma_10}{{14f}{12}{Support vectors for gamma = 10\relax }{figure.caption.34}{}}
\newlabel{sub@fig:svm-support_vectors_gamma_10}{{f}{12}{Support vectors for gamma = 10\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Decision regions and respective support vectors for different values of hyperparameter gamma in kernel function\relax }}{12}{figure.caption.34}\protected@file@percent }
\newlabel{fig:svm-decision_regions_support_vectos_gamma}{{14}{12}{Decision regions and respective support vectors for different values of hyperparameter gamma in kernel function\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{We now change the parameter gamma of the Radial Basis Function (RBF) kernel. Values of 0.1, 1 and 10 were used. Figure \ref  {fig:svm-decision_regions_support_vectos_gamma} shows the decision regions and the support vectors identified.}{12}{figure.caption.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A similar effect observed with hyperparameter C can be seen here. For small values of gamma, we have a very soft margin, producing a rough classifier and suggesting an underfitting. For gamma equal to 10, the model bent itself to fit almost all data. This leads to a significant generalization loss, indicating an overfitting. Gamma = 1.0 seems to generate a reasonable classifier.}{13}{section*.35}\protected@file@percent }
