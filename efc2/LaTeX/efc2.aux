\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:unicamp}{{\caption@xref {fig:unicamp}{ on input line 38}}{1}{}{figure.caption.1}{}}
\newlabel{sub@fig:unicamp}{{}{1}{}{figure.caption.1}{}}
\newlabel{fig:feec}{{\caption@xref {fig:feec}{ on input line 45}}{1}{}{figure.caption.1}{}}
\newlabel{sub@fig:feec}{{}{1}{}{figure.caption.1}{}}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Source files}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{All code cited and all figures showed here can be found at the following GitHub repository:\\ \url  {https://github.com/ito-rafael/IA006C-MachineLearning/tree/master/efc2}\\ In this repository, one can found the following files:\\}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_pre-ex1'' plots the histograms for the exercise 1 and it is used for data visualization. It shows the input features histograms for the raw data and after a data standardization. Also, it shows the correlation between these data.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_ex1\_binary\_classification'' effectively implements the logistic regression used to perform a binary classification proposed in exercise 1.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebooks ``efc2\_ex2\_multiclass\_classification'' and ``efc2\_ex2\_knn'' implements the algorithms to perform a multiclass classification proposed in exercise 2. The former one uses the softmax approach while the latter one implements the K-Nearest Neighbors (KNN) algorithm.}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part 1 - Binary Classification}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}a) Input features characteristics analysis considering the histograms and correlation measures between them.}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The first thing we did for this item, was to plot the histograms of the data before and after standardization. The histograms for the raw data can be seen in Figure \ref  {fig:pre-ex1-raw_histograms} and the histograms for the standardized data can be seen in Figure \ref  {fig:pre-ex1-std_histograms}. To perform the standardization, the StandardScaler class provided by the scikit-learn library was used.}{3}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Correlation between input features\relax }}{3}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pre-ex1-correlation}{{4}{3}{Correlation between input features\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{In the Figure \ref  {fig:pre-ex1-correlation} we can see the correlation matrix, with values between 0 and 1, mapped in colors.}{3}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sub_raw_1}{{2a}{4}{sd\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_1}{{a}{4}{sd\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_2}{{2b}{4}{median\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_2}{{b}{4}{median\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_3}{{2c}{4}{Q25\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_3}{{c}{4}{Q25\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_4}{{2d}{4}{Q75\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_4}{{d}{4}{Q75\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_5}{{2e}{4}{IQR\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_5}{{e}{4}{IQR\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_6}{{2f}{4}{skew\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_6}{{f}{4}{skew\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_7}{{2g}{4}{kurt\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_7}{{g}{4}{kurt\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_8}{{2h}{4}{sp ent\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_8}{{h}{4}{sp ent\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_9}{{2i}{4}{smf\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_9}{{i}{4}{smf\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_10}{{2j}{4}{mode\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_10}{{j}{4}{mode\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_11}{{2k}{4}{centroid\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_11}{{k}{4}{centroid\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_12}{{2l}{4}{meanfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_12}{{l}{4}{meanfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_13}{{2m}{4}{minfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_13}{{m}{4}{minfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_14}{{2n}{4}{maxfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_14}{{n}{4}{maxfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_15}{{2o}{4}{meandom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_15}{{o}{4}{meandom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_16}{{2p}{4}{mindom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_16}{{p}{4}{mindom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_17}{{2q}{4}{maxdom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_17}{{q}{4}{maxdom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_18}{{2r}{4}{dfrange\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_18}{{r}{4}{dfrange\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_19}{{2s}{4}{modindx\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_19}{{s}{4}{modindx\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histograms of raw input features\relax }}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig:pre-ex1-raw_histograms}{{2}{4}{Histograms of raw input features\relax }{figure.caption.7}{}}
\newlabel{fig:sub_std_1}{{3a}{5}{sd\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_1}{{a}{5}{sd\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_2}{{3b}{5}{median\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_2}{{b}{5}{median\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_3}{{3c}{5}{Q25\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_3}{{c}{5}{Q25\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_4}{{3d}{5}{Q75\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_4}{{d}{5}{Q75\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_5}{{3e}{5}{IQR\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_5}{{e}{5}{IQR\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_6}{{3f}{5}{skew\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_6}{{f}{5}{skew\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_7}{{3g}{5}{kurt\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_7}{{g}{5}{kurt\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_8}{{3h}{5}{sp ent\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_8}{{h}{5}{sp ent\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_9}{{3i}{5}{smf\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_9}{{i}{5}{smf\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_10}{{3j}{5}{mode\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_10}{{j}{5}{mode\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_11}{{3k}{5}{centroid\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_11}{{k}{5}{centroid\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_12}{{3l}{5}{meanfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_12}{{l}{5}{meanfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_13}{{3m}{5}{minfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_13}{{m}{5}{minfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_14}{{3n}{5}{maxfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_14}{{n}{5}{maxfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_15}{{3o}{5}{meandom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_15}{{o}{5}{meandom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_16}{{3p}{5}{mindom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_16}{{p}{5}{mindom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_17}{{3q}{5}{maxdom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_17}{{q}{5}{maxdom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_18}{{3r}{5}{dfrange\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_18}{{r}{5}{dfrange\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_19}{{3s}{5}{modindx\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_19}{{s}{5}{modindx\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Histograms of input features after data standardization\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:pre-ex1-std_histograms}{{3}{5}{Histograms of input features after data standardization\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Logistic function\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic}{{5}{6}{Logistic function\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{In the first notebook we also plotted the logistic function, used in the model proposed for binary classification. The Figure \ref  {fig:pre-ex1-logistic} shows the logistic function in an interval of -2 and 2.}{6}{figure.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparasion between hyperbolic tangent and logistic function(scaled and with offset)\relax }}{6}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic_tanh}{{6}{6}{Comparasion between hyperbolic tangent and logistic function(scaled and with offset)\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Before actually starting the binary classification proposed in exercise 1, one fact can be noted when comparing the logistic function (used in this exercise) and the non-linear function (hyperbolic tangent) used to transform the data in the exercise 2 of the previous list of exercises EFC1. The shape of these functions are very similar. Just to visualize the shape of the functions, the two functions are plotted in the same Figure \ref  {fig:pre-ex1-logistic_tanh}. Here, the logistic function was multiplied by two, and subtracted by 1 in order to show the similar shape with the hyperbolic tangent (but with a different scale).}{6}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}b) Logistic regression, ROC and F1-score curves.}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In this subsection the binary classifier is actually built. Here we are using the shuffle function provided by the scikit-learn library. First, we separate the data according to the label. We do this to ensure that we have the same rate of both classes in the training set and also in the test set. The data in both groups are then shuffled.}{7}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error progression in training\relax }}{7}{figure.caption.17}\protected@file@percent }
\newlabel{fig:ex1-error}{{7}{7}{Error progression in training\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{The training step is performed until the difference between successive iteration errors is under a provided tolerance. We are using a tolerance of $1.10^{-6}$, which means that when the error between the real and the predicted class for a iteration is less than the error in the previous iteration minus this tolerance, we should stop the training process. The error progression with the number of epochs is shown in Figure \ref  {fig:ex1-error}. We are considering the whole batch of samples for each step during the training.}{7}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces ROC curve\relax }}{7}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ex1-roc}{{8}{7}{ROC curve\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Now we are going to plot the receiver operating characteristic (ROC) curve and the F1-score for the classifier. For this, we are using the metrics provided by the scikit-learn library. The ROC curve is plotted with the recall (true positive rate - tpr) being the y axis and the false positive rate (fpr) being the x axis. The ROC curve can be seen in the Figure \ref  {fig:ex1-roc}. We can see that the ROC curve is located at a high part of the graph as well it is to the left, indicating a good performance.}{8}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces F1-score curve\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ex1-f1_score}{{9}{8}{F1-score curve\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{We now change the threshold used to classify a sample with the positive or negative class. We use values of threshold from 0 to 1 with step of 0.001, i.e., we are using 1000 thresholds values. For each value of threshold we calculate the F1-score. Figure \ref  {fig:ex1-f1_score} shows this plot.}{8}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}c) Threshold, confusion matrix and accuracy.}{8}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Zoom in F1-score curve\relax }}{8}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ex1-f1_score_zoom}{{10}{8}{Zoom in F1-score curve\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{If we give a zoom in Figure \ref  {fig:ex1-f1_score} highlighting the points near the maximum of the curve, as shown in Figure \ref  {fig:ex1-f1_score_zoom}, we can choose the most appropriate threshold value. Knowing that the F1-score can be interpreted as a weighted average of the precision and recall (with 1 being the best value and 0 being the worst), we then choose for the threshold which maximizes the F1-score. This gives us a threshold of 0.689.}{9}{figure.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Part 2 - Multiclass Classification}{10}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We start this exercise by performing a one-hot encoding with the data. This way we can represent the categorical variables in a binary way. This encoding is defined as follows:}{10}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}a) Logistic regression (using softmax approach)}{10}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The approach used to implement the multiclass classifier was the softmax. All operations involved was performed in the matrix way, in order to try to improve the computational costs involved.}{10}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A weight array with K+1 elements (in this case we have K = 561 features) is randomly initiated according to a uniform distribution between -1 and 1 for each class, as can be seen:}{10}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As we have six different classes, the W matrix can be generated as follows:}{10}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The stop criteria adopted was a tolerance of $2.10^{-2}$ for the error in a given iteration. The function used to calculate the error was the zero-one loss, where a 0 is assigned when the class was correctly classified, and 1 when it was not. We then calculate the zero-one loss for all data in the training set, sum all of them and divide by the number of inputs (7352 in this case).}{10}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Zero-one loss curve\relax }}{11}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss}{{11}{11}{Zero-one loss curve\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {paragraph}{The variation of the error with the number of epochs is shown in the Figure \ref  {fig:ex2-a-zero_one_loss}.}{11}{figure.caption.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Zoom in zero-one loss curve\relax }}{11}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss_zoom}{{12}{11}{Zoom in zero-one loss curve\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {paragraph}{In the Figure \ref  {fig:ex2-a-zero_one_loss_zoom} we can see a zoom of the previous curve focusing in the beginning of the curve. As we can see, the initial error is very high (bigger than 0.9) due to the randomly generated weights.}{11}{figure.caption.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now it is time to measure the performance of the classifier. To do this, we again are using the metrics provided by the scikit-learn library. We are using the ``\textit  {confusion\_matrix}'' and ``\textit  {f1\_score}'' functions.}{11}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}b) k-Nearest Neighbors}{11}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For the k-Nearest Neighbors (kNN) method, we use a lazy learning method. For this, the first thing we should do is to calculate somehow the distance between the new data and all the data from the training set. The distance metric used here was the Minkowski.}{12}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Minkowski distance:}{12}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For \textit  {p = 2}, we have the Euclidian distance:}{12}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In order to reduce the calculation costs, the distance between all data from the training set and all data in the test set were calculated in the matrix form. A distance matrix $D$ was created for this, as can be seen as follows:}{12}{section*.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Zero-one loss curve (uniform weights\relax }}{12}{figure.caption.39}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform}{{13}{12}{Zero-one loss curve (uniform weights\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Zero-one loss curve (uniform weights\relax }}{13}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform_zoom}{{14}{13}{Zero-one loss curve (uniform weights\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {paragraph}{During the decision step two approaches were used. The first one presented here uses uniform weights, i.e., each k neighbors have the same weight when voting for their own class. The result for this approach can be seen in the Figure \ref  {fig:ex2-b-error_uniform}. In Figure \ref  {fig:ex2-b-error_uniform_zoom} we can see a zoom in the previous plot showing the value of k optimum = AAAAAAAAA.}{13}{figure.caption.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{13}{figure.caption.42}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance}{{15}{13}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{14}{figure.caption.43}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance_zoom}{{16}{14}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {paragraph}{The second approach uses distance weights. In this case, each vote is pondered according to the inverse of the distance. This means that when predicting the class for a new input, the class of a very similar data in the training set has more relevance than a data not so close to this new input. The result for this approach can be seen in the Figure \ref  {fig:ex2-b-error_distance}. In Figure \ref  {fig:ex2-b-error_uniform_zoom} we can see a zoom in the previous plot showing the value of k optimum = AAAAAAAAA.}{14}{figure.caption.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ COMPARASION BETWEEN softmax and kNN COMPARASION BETWEEN DISTANCE AND UNIFORM }{14}{section*.44}\protected@file@percent }
