\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:unicamp}{{\caption@xref {fig:unicamp}{ on input line 36}}{1}{}{figure.caption.1}{}}
\newlabel{sub@fig:unicamp}{{}{1}{}{figure.caption.1}{}}
\newlabel{fig:feec}{{\caption@xref {fig:feec}{ on input line 43}}{1}{}{figure.caption.1}{}}
\newlabel{sub@fig:feec}{{}{1}{}{figure.caption.1}{}}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Source files}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{All code cited and all figures showed here can be found at the following GitHub repository:\\ \url  {https://github.com/ito-rafael/IA006C-MachineLearning/tree/master/efc2}\\ In this repository, one can found the following files:\\}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_pre-ex1'' plots the histograms for the exercise 1 and it is used for data visualization. It shows the input features histograms for the raw data and after a data standardization. Also, it shows the correlation between these data.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_ex1\_binary\_classification'' effectively implements the logistic regression used to perform a binary classification proposed in exercise 1.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebooks ``efc2\_ex2\_multiclass\_classification'' and ``efc2\_ex2\_knn'' implements the algorithms to perform a multiclass classification proposed in exercise 2. The former one uses the softmax approach while the latter one implements the K-Nearest Neighbors (KNN) algorithm.}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part 1 - Binary Classification}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}a) Input features characteristics analysis considering the histograms and correlation measures between them.}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The first thing we did for this item, was to plot the histograms of the data before and after standardization. The histograms for the raw data can be seen in Figure \ref  {fig:pre-ex1-raw_histograms} and the histograms for the standardized data can be seen in Figure \ref  {fig:pre-ex1-std_histograms}. To perform the standardization, the StandardScaler class provided by the scikit-learn library was used.}{2}{figure.caption.8}\protected@file@percent }
\newlabel{fig:sub_raw_1}{{2a}{3}{sd\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_1}{{a}{3}{sd\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_2}{{2b}{3}{median\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_2}{{b}{3}{median\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_3}{{2c}{3}{Q25\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_3}{{c}{3}{Q25\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_4}{{2d}{3}{Q75\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_4}{{d}{3}{Q75\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_5}{{2e}{3}{IQR\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_5}{{e}{3}{IQR\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_6}{{2f}{3}{skew\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_6}{{f}{3}{skew\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_7}{{2g}{3}{kurt\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_7}{{g}{3}{kurt\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_8}{{2h}{3}{sp ent\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_8}{{h}{3}{sp ent\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_9}{{2i}{3}{smf\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_9}{{i}{3}{smf\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_10}{{2j}{3}{mode\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_10}{{j}{3}{mode\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_11}{{2k}{3}{centroid\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_11}{{k}{3}{centroid\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_12}{{2l}{3}{meanfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_12}{{l}{3}{meanfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_13}{{2m}{3}{minfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_13}{{m}{3}{minfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_14}{{2n}{3}{maxfun\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_14}{{n}{3}{maxfun\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_15}{{2o}{3}{meandom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_15}{{o}{3}{meandom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_16}{{2p}{3}{mindom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_16}{{p}{3}{mindom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_17}{{2q}{3}{maxdom\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_17}{{q}{3}{maxdom\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_18}{{2r}{3}{dfrange\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_18}{{r}{3}{dfrange\relax }{figure.caption.7}{}}
\newlabel{fig:sub_raw_19}{{2s}{3}{modindx\relax }{figure.caption.7}{}}
\newlabel{sub@fig:sub_raw_19}{{s}{3}{modindx\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histograms of raw input features\relax }}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:pre-ex1-raw_histograms}{{2}{3}{Histograms of raw input features\relax }{figure.caption.7}{}}
\newlabel{fig:sub_std_1}{{3a}{4}{sd\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_1}{{a}{4}{sd\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_2}{{3b}{4}{median\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_2}{{b}{4}{median\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_3}{{3c}{4}{Q25\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_3}{{c}{4}{Q25\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_4}{{3d}{4}{Q75\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_4}{{d}{4}{Q75\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_5}{{3e}{4}{IQR\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_5}{{e}{4}{IQR\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_6}{{3f}{4}{skew\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_6}{{f}{4}{skew\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_7}{{3g}{4}{kurt\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_7}{{g}{4}{kurt\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_8}{{3h}{4}{sp ent\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_8}{{h}{4}{sp ent\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_9}{{3i}{4}{smf\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_9}{{i}{4}{smf\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_10}{{3j}{4}{mode\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_10}{{j}{4}{mode\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_11}{{3k}{4}{centroid\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_11}{{k}{4}{centroid\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_12}{{3l}{4}{meanfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_12}{{l}{4}{meanfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_13}{{3m}{4}{minfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_13}{{m}{4}{minfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_14}{{3n}{4}{maxfun\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_14}{{n}{4}{maxfun\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_15}{{3o}{4}{meandom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_15}{{o}{4}{meandom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_16}{{3p}{4}{mindom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_16}{{p}{4}{mindom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_17}{{3q}{4}{maxdom\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_17}{{q}{4}{maxdom\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_18}{{3r}{4}{dfrange\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_18}{{r}{4}{dfrange\relax }{figure.caption.8}{}}
\newlabel{fig:sub_std_19}{{3s}{4}{modindx\relax }{figure.caption.8}{}}
\newlabel{sub@fig:sub_std_19}{{s}{4}{modindx\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Histograms of input features after data standardization\relax }}{4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:pre-ex1-std_histograms}{{3}{4}{Histograms of input features after data standardization\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Correlation between input features\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pre-ex1-correlation}{{4}{5}{Correlation between input features\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{In the Figure \ref  {fig:pre-ex1-correlation} we can see the correlation matrix, with values between 0 and 1, mapped in colors.}{5}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Logistic function\relax }}{5}{figure.caption.12}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic}{{5}{5}{Logistic function\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{In the first notebook we also plotted the logistic function, used in the model proposed for binary classification. The Figure \ref  {fig:pre-ex1-logistic} shows the logistic function in an interval of -2 and 2.}{6}{figure.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparasion between hyperbolic tangent and logistic function(scaled and with offset)\relax }}{6}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic_tanh}{{6}{6}{Comparasion between hyperbolic tangent and logistic function(scaled and with offset)\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Before actually starting the binary classification proposed in exercise 1, one fact can be noted when comparing the logistic function (used in this exercise) and the non-linear function (hyperbolic tangent) used to transform the data in the exercise 2 of the previous list of exercises EFC1. The shape of these functions are very similar. Just to visualize the shape of the functions, the two functions are plotted in the same Figure \ref  {fig:pre-ex1-logistic_tanh}. Here, the logistic function was multiplied by two, and subtracted by 1 in order to show the similar shape with the hyperbolic tangent (but with a different scale).}{6}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}b) Logistic regression, ROC and F1-score curves.}{6}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In this subsection the binary classifier is actually built. Here we are using the shuffle function provided by the scikit-learn library. First, we separate the data according to the label. We do this to ensure that we have the same rate of both classes in the training set and also in the test set. The data in both groups are then shuffled.}{6}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error progression in training\relax }}{7}{figure.caption.17}\protected@file@percent }
\newlabel{fig:ex1-error}{{7}{7}{Error progression in training\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{The training step is performed until the difference between successive iteration errors is under a provided tolerance. We are using a tolerance of $1.10^{-6}$, which means that when the error between the real and the predicted class for a iteration is less than the error in the previous iteration minus this tolerance, we should stop the training process. The error progression with the number of epochs is shown in Figure \ref  {fig:ex1-error}. We are considering the whole batch of samples for each step during the training.}{7}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces ROC curve\relax }}{7}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ex1-roc}{{8}{7}{ROC curve\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Now we are going to plot the receiver operating characteristic (ROC) curve and the F1-score for the classifier. For this, we are using the metrics provided by the scikit-learn library. The ROC curve is plotted with the recall (true positive rate - tpr) being the y axis and the false positive rate (fpr) being the x axis. The ROC curve can be seen in the Figure \ref  {fig:ex1-roc}. We can see that the ROC curve is located at a high part of the graph as well it is to the left, indicating a good performance.}{7}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces F1-score curve\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ex1-f1_score}{{9}{8}{F1-score curve\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{We now change the threshold used to classify a sample with the positive or negative class. We use values of threshold from 0 to 1 with step of 0.001, i.e., we are using 1000 thresholds values. For each value of threshold we calculate the F1-score. Figure \ref  {fig:ex1-f1_score} shows this plot.}{8}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}c) Threshold, confusion matrix and accuracy.}{8}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Zoom in F1-score curve\relax }}{8}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ex1-f1_score_zoom}{{10}{8}{Zoom in F1-score curve\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{If we give a zoom in Figure \ref  {fig:ex1-f1_score} highlighting the points near the maximum of the curve, as shown in Figure \ref  {fig:ex1-f1_score_zoom}, we can choose the most appropriate threshold value. Knowing that the F1-score can be interpreted as a weighted average of the precision and recall (with 1 being the best value and 0 being the worst), we then choose for the threshold which maximizes the F1-score. This gives us a threshold of 0.689.}{8}{figure.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Part 2 - Multiclass Classification}{9}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}a) Logistic regression (using softmax approach)}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We start this exercise by }{9}{section*.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Zero-one loss curve\relax }}{9}{figure.caption.25}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss}{{11}{9}{Zero-one loss curve\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Zoom in zero-one loss curve\relax }}{9}{figure.caption.26}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss_zoom}{{12}{9}{Zoom in zero-one loss curve\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}b) K-Nearest Neighbors}{10}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Zero-one loss curve (uniform weights\relax }}{10}{figure.caption.27}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform_0}{{13}{10}{Zero-one loss curve (uniform weights\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Zero-one loss curve (uniform weights\relax }}{10}{figure.caption.28}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform_1}{{14}{10}{Zero-one loss curve (uniform weights\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Zero-one loss curve (uniform weights\relax }}{11}{figure.caption.29}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform_2}{{15}{11}{Zero-one loss curve (uniform weights\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{11}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance_0}{{16}{11}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{12}{figure.caption.31}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance_1}{{17}{12}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{12}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance_2}{{18}{12}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.32}{}}
