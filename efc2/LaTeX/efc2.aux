\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Source files}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{All code cited and all figures showed here can be found at the following GitHub repository:\\ \url  {https://github.com/ito-rafael/IA006C-MachineLearning/tree/master/efc2}\\ In this repository, one can found the following files:\\}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_pre-ex1'' plots the histograms for the exercise 1 and it is used for data visualization. It shows the input features histograms for the raw data and after a data standardization. Also, it shows the correlation between these data.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebook ``efc2\_ex1\_binary\_classification'' effectively implements the logistic regression used to perform a binary classification proposed in exercise 1.}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The notebooks ``efc2\_ex2\_multiclass\_classification'' and ``efc2\_ex2\_knn'' implements the algorithms to perform a multiclass classification proposed in exercise 2. The former one uses the softmax approach while the latter one implements the K-Nearest Neighbors (kNN) algorithm.}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part 1 - Binary Classification}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}a) Input features characteristics analysis considering the histograms and correlation measures between them.}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The first thing we did for this item, was to plot the histograms of the data before and after standardization. The histograms for the raw data can be seen in Figure \ref  {fig:pre-ex1-raw_histograms} and the histograms for the standardized data can be seen in Figure \ref  {fig:pre-ex1-std_histograms}. To perform the standardization, the StandardScaler class provided by the scikit-learn library was used.}{2}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {fig:pre-ex1-sbs_histograms} shows the histograms of the input features separated according to the labels. The orange plot represents the label 0 (female class) and the blue plot represents the label 1 (male class).}{2}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As we can see in Figure \ref  {fig:pre-ex1-sbs_histograms}, some features may be visually very informative about the class. For example, features ``mean fun'', ``Q25'' and ``IQR'' can be very informative and show the classes in a distinct way. Features like ``sd'', ``sp ent'', ``smf'' and ``mode'' can bring some information about the class, but they are not so obvious, since both classes are showed in a specific value of the histograms. In some cases, it is not possible to separate the classes only by visually analyzing the histograms of the features in a straightforward way, like in the case of the features ``Q75'', ``maxfun'' or ``modindx'', since the data has values in a similar range bor both classes.}{2}{section*.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub_raw_1}{{2a}{3}{sd\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_1}{{a}{3}{sd\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_2}{{2b}{3}{median\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_2}{{b}{3}{median\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_3}{{2c}{3}{Q25\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_3}{{c}{3}{Q25\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_4}{{2d}{3}{Q75\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_4}{{d}{3}{Q75\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_5}{{2e}{3}{IQR\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_5}{{e}{3}{IQR\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_6}{{2f}{3}{skew\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_6}{{f}{3}{skew\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_7}{{2g}{3}{kurt\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_7}{{g}{3}{kurt\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_8}{{2h}{3}{sp ent\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_8}{{h}{3}{sp ent\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_9}{{2i}{3}{smf\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_9}{{i}{3}{smf\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_10}{{2j}{3}{mode\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_10}{{j}{3}{mode\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_11}{{2k}{3}{centroid\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_11}{{k}{3}{centroid\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_12}{{2l}{3}{meanfun\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_12}{{l}{3}{meanfun\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_13}{{2m}{3}{minfun\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_13}{{m}{3}{minfun\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_14}{{2n}{3}{maxfun\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_14}{{n}{3}{maxfun\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_15}{{2o}{3}{meandom\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_15}{{o}{3}{meandom\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_16}{{2p}{3}{mindom\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_16}{{p}{3}{mindom\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_17}{{2q}{3}{maxdom\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_17}{{q}{3}{maxdom\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_18}{{2r}{3}{dfrange\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_18}{{r}{3}{dfrange\relax }{figure.caption.9}{}}
\newlabel{fig:sub_raw_19}{{2s}{3}{modindx\relax }{figure.caption.9}{}}
\newlabel{sub@fig:sub_raw_19}{{s}{3}{modindx\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histograms of raw input features\relax }}{3}{figure.caption.9}\protected@file@percent }
\newlabel{fig:pre-ex1-raw_histograms}{{2}{3}{Histograms of raw input features\relax }{figure.caption.9}{}}
\newlabel{fig:sub_std_1}{{3a}{4}{sd\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_1}{{a}{4}{sd\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_2}{{3b}{4}{median\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_2}{{b}{4}{median\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_3}{{3c}{4}{Q25\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_3}{{c}{4}{Q25\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_4}{{3d}{4}{Q75\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_4}{{d}{4}{Q75\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_5}{{3e}{4}{IQR\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_5}{{e}{4}{IQR\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_6}{{3f}{4}{skew\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_6}{{f}{4}{skew\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_7}{{3g}{4}{kurt\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_7}{{g}{4}{kurt\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_8}{{3h}{4}{sp ent\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_8}{{h}{4}{sp ent\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_9}{{3i}{4}{smf\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_9}{{i}{4}{smf\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_10}{{3j}{4}{mode\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_10}{{j}{4}{mode\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_11}{{3k}{4}{centroid\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_11}{{k}{4}{centroid\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_12}{{3l}{4}{meanfun\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_12}{{l}{4}{meanfun\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_13}{{3m}{4}{minfun\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_13}{{m}{4}{minfun\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_14}{{3n}{4}{maxfun\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_14}{{n}{4}{maxfun\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_15}{{3o}{4}{meandom\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_15}{{o}{4}{meandom\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_16}{{3p}{4}{mindom\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_16}{{p}{4}{mindom\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_17}{{3q}{4}{maxdom\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_17}{{q}{4}{maxdom\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_18}{{3r}{4}{dfrange\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_18}{{r}{4}{dfrange\relax }{figure.caption.10}{}}
\newlabel{fig:sub_std_19}{{3s}{4}{modindx\relax }{figure.caption.10}{}}
\newlabel{sub@fig:sub_std_19}{{s}{4}{modindx\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Histograms of input features after data standardization\relax }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pre-ex1-std_histograms}{{3}{4}{Histograms of input features after data standardization\relax }{figure.caption.10}{}}
\newlabel{fig:sub_sbs_1}{{4a}{5}{sd\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_1}{{a}{5}{sd\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_2}{{4b}{5}{median\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_2}{{b}{5}{median\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_3}{{4c}{5}{Q25\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_3}{{c}{5}{Q25\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_4}{{4d}{5}{Q75\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_4}{{d}{5}{Q75\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_5}{{4e}{5}{IQR\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_5}{{e}{5}{IQR\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_6}{{4f}{5}{skew\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_6}{{f}{5}{skew\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_7}{{4g}{5}{kurt\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_7}{{g}{5}{kurt\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_8}{{4h}{5}{sp ent\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_8}{{h}{5}{sp ent\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_9}{{4i}{5}{smf\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_9}{{i}{5}{smf\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_10}{{4j}{5}{mode\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_10}{{j}{5}{mode\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_11}{{4k}{5}{centroid\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_11}{{k}{5}{centroid\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_12}{{4l}{5}{meanfun\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_12}{{l}{5}{meanfun\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_13}{{4m}{5}{minfun\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_13}{{m}{5}{minfun\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_14}{{4n}{5}{maxfun\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_14}{{n}{5}{maxfun\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_15}{{4o}{5}{meandom\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_15}{{o}{5}{meandom\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_16}{{4p}{5}{mindom\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_16}{{p}{5}{mindom\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_17}{{4q}{5}{maxdom\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_17}{{q}{5}{maxdom\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_18}{{4r}{5}{dfrange\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_18}{{r}{5}{dfrange\relax }{figure.caption.11}{}}
\newlabel{fig:sub_sbs_19}{{4s}{5}{modindx\relax }{figure.caption.11}{}}
\newlabel{sub@fig:sub_sbs_19}{{s}{5}{modindx\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Histograms of input features after data standardization\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:pre-ex1-sbs_histograms}{{4}{5}{Histograms of input features after data standardization\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{In the Figure \ref  {fig:pre-ex1-correlation} we can see the correlation matrix, with values between 0 and 1, mapped in colors. We can see from this image, that there are cases where a feature is highly correlated to another and also cases in which no significative correlation was measured. For example, features like ``skew'' and ``kurt'', ``maxdom'' and ``dfrange'', ``centroid'', ``Q25'' and ``median'' have a high correlation. In fact, if we look into these two first pairs of features, we can see that the shape of the histograms are very similar.}{6}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Correlation between input features\relax }}{6}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pre-ex1-correlation}{{5}{6}{Correlation between input features\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{On the other hand, features like ``sd'' and ``Q25'', ``IQR'' and Q25, ``centroid'' and ``sfm'', presents a low correlation. An intuition about this can be inferred from the fact that for all these pair of features, the shape of the plots in the histograms are kind of mirrored. For example, for the ``IQR'' feature the orange data is plotted before the blue data, while in ``Q25'' the opposite happens. One thing to be noted is that the matrix presents a symmetry with respect to the diagonal of this matrix, indicating that the correlation of A with respect to B is the same of the correlation of B with respect to A.}{6}{figure.caption.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Logistic function\relax }}{7}{figure.caption.16}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic}{{6}{7}{Logistic function\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{In the first notebook we also plotted the logistic function, used in the model proposed for binary classification. The Figure \ref  {fig:pre-ex1-logistic} shows the logistic function in an interval of -2 and 2.}{7}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparasion between hyperbolic tangent and logistic function (scaled and with offset)\relax }}{7}{figure.caption.18}\protected@file@percent }
\newlabel{fig:pre-ex1-logistic_tanh}{{7}{7}{Comparasion between hyperbolic tangent and logistic function (scaled and with offset)\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Before actually starting the binary classification proposed in exercise 1, one fact can be noted when comparing the logistic function (used in this exercise) and the non-linear function (hyperbolic tangent) used to transform the data in the exercise 2 of the previous list of exercises EFC1. The shape of these functions are very similar. Just to visualize the shape of the functions, the two functions are plotted in the same Figure \ref  {fig:pre-ex1-logistic_tanh}. Here, the logistic function was multiplied by two, and subtracted by one in order to show the similar shape with the hyperbolic tangent (but with a different scale).}{7}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}b) Logistic regression, ROC and F1-score curves.}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In this subsection the binary classifier is actually coded. Here we are using the \textit  {shuffle} function provided by the scikit-learn library. First, we separate the data according to the label. We do this to ensure that we have the same rate of both classes in the training set and also in the test set. The data in both groups are then shuffled.}{8}{section*.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Error progression in training\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ex1-error}{{8}{8}{Error progression in training\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{The training step is performed until the difference between successive iteration errors is under a provided tolerance. RMSE was chosen to be the error criteria. We are using a tolerance of $1.10^{-6}$, which means that when the error between the real and the predicted class for a iteration is less than the error in the previous iteration minus this tolerance, we should stop the training process. The error progression with the number of epochs is shown in Figure \ref  {fig:ex1-error}. We are considering the whole batch of samples for each step during the training.}{8}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces ROC curve\relax }}{9}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ex1-roc}{{9}{9}{ROC curve\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Now we are going to plot the receiver operating characteristic (ROC) curve and the F1-score for the classifier. For this, we are using the metrics provided by the scikit-learn library. The ROC curve is plotted with the recall (true positive rate, tpr) being the y axis and the false positive rate (fpr) being the x axis. The ROC curve can be seen in the Figure \ref  {fig:ex1-roc}. We can see that the ROC curve is located at a high part of the graph as well it is to the left, indicating a good performance.}{9}{figure.caption.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces F1-score curve\relax }}{9}{figure.caption.25}\protected@file@percent }
\newlabel{fig:ex1-f1_score}{{10}{9}{F1-score curve\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{We now change the threshold used to classify a sample with the positive or negative class. We used values of threshold from 0 to 1 with step of 0.001, i.e., we are using 1000 different thresholds values. For each value of threshold we calculated the F1-score. Figure \ref  {fig:ex1-f1_score} shows this plot.}{9}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}c) Threshold, confusion matrix and accuracy.}{10}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Zoom in F1-score curve\relax }}{10}{figure.caption.27}\protected@file@percent }
\newlabel{fig:ex1-f1_score_zoom}{{11}{10}{Zoom in F1-score curve\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {paragraph}{If we give a zoom in Figure \ref  {fig:ex1-f1_score} highlighting the points near the maximum of the curve, as shown in Figure \ref  {fig:ex1-f1_score_zoom}, we can choose the most appropriate threshold value. Knowing that the F1-score can be interpreted as a weighted average of the precision and recall (with 1 being the best value and 0 being the worst), we then choose for the threshold which maximizes the F1-score. This gives us a threshold of 0.689.}{10}{figure.caption.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Confusion Matrix for binary classification (logistic regression)\relax }}{11}{figure.caption.29}\protected@file@percent }
\newlabel{fig:ex1-cm_raw}{{12}{11}{Confusion Matrix for binary classification (logistic regression)\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Confusion Matrix normalized for binary classification (logistic regression)\relax }}{11}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ex1-cm_norm}{{13}{11}{Confusion Matrix normalized for binary classification (logistic regression)\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Using the chosen threshold (with value 0.689) we now plot the confusion matrix. The Figure \ref  {fig:ex1-cm_raw} shows the confusion matrix with raw values and the Figure \ref  {fig:ex1-cm_norm} shows the one with normalized values.}{11}{figure.caption.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Finally, we can calculate the accuracy for this classifier, which gives us a value of 0.9826, indicating a good performance.}{11}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Part 2 - Multiclass Classification}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We start this exercise by performing a one-hot encoding with the data. This way we can represent the categorical variables in a binary way. This encoding is defined as follows:}{12}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}a) Logistic regression (using softmax approach)}{12}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The approach used to implement the multiclass classifier was the softmax function. All operations involved was performed in the matrix way, in order to try to improve the computational costs involved.}{12}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A weight array with K+1 elements (in this case we have K = 561 features) is randomly initiated according to a uniform distribution between -1 and 1 for each class, as can be seen:}{12}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As we have six different classes, the W matrix can be generated as follows:}{12}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The stop criteria adopted was a tolerance of $2.10^{-2}$ for the error in a given iteration. The function used to calculate the error was the zero-one loss, where a 0 is assigned when the class was correctly classified, and 1 when it was not. We then calculate the zero-one loss for all data in the training set, sum all of them and divide by the number of inputs (7352 in this case).}{12}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Zero-one loss curve\relax }}{13}{figure.caption.38}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss}{{14}{13}{Zero-one loss curve\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {paragraph}{The variation of the error with the number of epochs is shown in the Figure \ref  {fig:ex2-a-zero_one_loss}.}{13}{figure.caption.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Zoom in zero-one loss curve\relax }}{13}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ex2-a-zero_one_loss_zoom}{{15}{13}{Zoom in zero-one loss curve\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {paragraph}{In the Figure \ref  {fig:ex2-a-zero_one_loss_zoom} we can see a zoom of the previous curve focusing in the beginning of the curve. As we can see, the initial error is very high (bigger than 0.9) due to the randomly generated weights.}{13}{figure.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now it is time to measure the performance of the classifier. To plot the confusion matrix we are using the ``\textit  {plot\_confusion\_matrix}'' described in the notebook, and the ``\textit  {f1\_score}'' function provided by the scikit-learn library.}{13}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Confusion Matrix using softmax model\relax }}{14}{figure.caption.43}\protected@file@percent }
\newlabel{fig:ex2-a-cm_raw}{{16}{14}{Confusion Matrix using softmax model\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Confusion Matrix with normalized values using softmax model\relax }}{14}{figure.caption.44}\protected@file@percent }
\newlabel{fig:ex2-a-cm_norm}{{17}{14}{Confusion Matrix with normalized values using softmax model\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Figures \ref  {fig:ex2-a-cm_raw} and \ref  {fig:ex2-a-cm_norm} shows the confusion matrix with raw and normalized values, respectively.}{14}{figure.caption.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Having the confusion matrix, we can calculate metrics to measure the performance of the classifier and compare with the k-Nearest Neighbor, the next model we will validate. The zero-one loss and the F1-score obtained are:}{14}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}b) k-Nearest Neighbors}{15}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For the k-Nearest Neighbors (kNN) method, we use a lazy learning method. For this, the first thing we should do is to calculate somehow the distance between the new data and all the data from the training set. The distance metric used here was the Minkowski.}{15}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Minkowski distance:}{15}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For \textit  {p = 2}, we have the Euclidian distance:}{15}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In order to reduce the calculation costs, the distance between all data from the training set and all data in the validation set were calculated in the matrix form. A distance matrix $D$ was created for this, as can be seen as follows:}{15}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Uniform weights}{15}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Zero-one loss curve (uniform weights)\relax }}{15}{figure.caption.51}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform}{{18}{15}{Zero-one loss curve (uniform weights)\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Zero-one loss curve (uniform weights)\relax }}{16}{figure.caption.52}\protected@file@percent }
\newlabel{fig:ex2-b-error_uniform_zoom}{{19}{16}{Zero-one loss curve (uniform weights)\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {paragraph}{During the decision step two approaches were used. The first one presented here uses uniform weights, i.e., each k neighbors have the same weight when voting for their own class. The result for this approach can be seen in the Figure \ref  {fig:ex2-b-error_uniform}. In Figure \ref  {fig:ex2-b-error_uniform_zoom} we can see a zoom in the previous plot showing the value of k optimum = 5, which minimizes the loss function.}{16}{figure.caption.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Confusion Matrix for uniform weights\relax }}{16}{figure.caption.54}\protected@file@percent }
\newlabel{fig:ex2-b-cm_uniform}{{20}{16}{Confusion Matrix for uniform weights\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Confusion Matrix normalized for uniform weights\relax }}{17}{figure.caption.55}\protected@file@percent }
\newlabel{fig:ex2-b-cm_uniform_normalized}{{21}{17}{Confusion Matrix normalized for uniform weights\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {paragraph}{After finding the best value for the hyperparameter k = 5, we now apply the model to the test data. The results can be seen in Figures \ref  {fig:ex2-b-cm_uniform} and \ref  {fig:ex2-b-cm_uniform_normalized}, which shows the confusion matrix with raw values and with normalized values, respectively.}{17}{figure.caption.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We then use metrics to measure the performance of the classifier. The results obtained are:}{17}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Distance weights}{18}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{18}{figure.caption.58}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance}{{22}{18}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Zero-one loss curve (inversely proportional to distance weights\relax }}{18}{figure.caption.59}\protected@file@percent }
\newlabel{fig:ex2-b-error_distance_zoom}{{23}{18}{Zero-one loss curve (inversely proportional to distance weights\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {paragraph}{The second approach uses distance weights. In this case, each vote is pondered according to the inverse of the distance. This means that when predicting the class for a new input, the class of a very similar data in the training set has more relevance than a data not so close to this new input. The result for this approach can be seen in the Figure \ref  {fig:ex2-b-error_distance}. In Figure \ref  {fig:ex2-b-error_distance_zoom} we can see a zoom in the previous plot showing the value of k optimum = 6, which minimizes the loss function.}{18}{figure.caption.59}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Confusion Matrix for uniform weights\relax }}{19}{figure.caption.61}\protected@file@percent }
\newlabel{fig:ex2-b-cm_distance}{{24}{19}{Confusion Matrix for uniform weights\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Confusion Matrix for distance weights\relax }}{19}{figure.caption.62}\protected@file@percent }
\newlabel{fig:ex2-b-cm_distance_normalized}{{25}{19}{Confusion Matrix for distance weights\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {paragraph}{After finding the best value for the hyperparameter k = 6, we now apply the model to the test data. The results can be seen in Figures \ref  {fig:ex2-b-cm_distance} and \ref  {fig:ex2-b-cm_distance_normalized}, which shows the confusion matrix with raw values and with normalized values, respectively.}{19}{figure.caption.62}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The last thing we did was to use metrics to measure the performance of the classifier. The results obtained are:}{19}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results and conclusion}{20}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of results between softmax and kNN models for multiclass classification\relax }}{20}{table.caption.65}\protected@file@percent }
\newlabel{tab:results}{{1}{20}{Comparison of results between softmax and kNN models for multiclass classification\relax }{table.caption.65}{}}
\@writefile{toc}{\contentsline {paragraph}{We now can compare the performance between the two models proposed in a multiclass classification scenario. For the k-Nearest Neighbors, two approaches were used: one considering uniform weights for the neighbors and another one considering distance weights. Table \ref  {tab:results} shows a summary of the results obtained:}{20}{table.caption.65}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{As expected, the softmax model is more robust. It presents the smallest error and the highest value for the F1-score, both micro and micro, showing that the overall classification is good (micro) and also the classification in each individual class (macro).}{20}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The kNN model, however, it does not present bad results when comparing to the softmax model. Actually, the results are quite reasonable, close to the ones obtained with the softmax function, which indicates that depending on the dataset and the problem, it can be an easy to implement and low cost alternative, considering that there is no training in this method, but only distance measures and comparison between them.}{20}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Still talking about the kNN model, when comparing the two approaches used, the distance weights has a superior performance than the uniform weights, even though the number of neighbors used are similar (5 for the uniform approach and 6 for the distance approach, chose by a cross-validation method).}{20}{section*.68}\protected@file@percent }
